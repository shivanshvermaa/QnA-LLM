{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from datetime import date\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "#Transformers\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "from transformers import AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "#Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "#PEFT\n",
    "from peft import LoraConfig\n",
    "from peft import PeftConfig\n",
    "from peft import PeftModel\n",
    "from peft import get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "#SFTT\n",
    "from trl import SFTTrainer\n",
    "\n",
    "#NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.cuda.empty_cache()\n",
    "cache_dir = \"/work/LitArt/cache\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "dataset_name = \"ccdv/govreport-summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to print the number of trainable parameters in the given model\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable %: {100 * trainable_params / all_param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name,cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['report', 'summary'],\n",
       "    num_rows: 17517\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_length(row):\n",
    "    row[\"length\"] = len(word_tokenize(row[\"report\"]))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(find_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].filter( lambda row: row[\"length\"] <= 4000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['report', 'summary', 'length'],\n",
       "    num_rows: 3476\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bnb Config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, #4bit quantizaition - load_in_4bit is used to load models in 4-bit quantization \n",
    "    bnb_4bit_use_double_quant=True, #nested quantization technique for even greater memory efficiency without sacrificing performance. This technique has proven beneficial, especially when fine-tuning large models\n",
    "    bnb_4bit_quant_type=\"nf4\", #quantization type used is 4 bit Normal Float Quantization- The NF4 data type is designed for weights initialized using a normal distribution\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, #modify the data type used during computation. This can result in speed improvements. \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,cache_dir=cache_dir)\n",
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_input(example,tokenizer):\n",
    "\n",
    "    prompt_start = \"Summarize the following government report : \\n\"\n",
    "    prompt_end = \"\\n\\nSummary:\"\n",
    "\n",
    "    prompt = [prompt_start + dialogue + prompt_end for dialogue in example[\"report\"]]\n",
    "\n",
    "    example[\"input_ids\"] = tokenizer(prompt ,max_length=4096 , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "    example[\"labels\"] = tokenizer(example[\"summary\"] , max_length=256 , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be20d81dbd70424bb8a4b01a97593182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3476 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = train_dataset.shuffle().map(tokenize_input, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['report', 'summary', 'length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d39a713a1e4169a22a12de86e94ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loading the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                    device_map=\"auto\",\n",
    "                                                    trust_remote_code=True,\n",
    "                                                    quantization_config=bnb_config,\n",
    "                                                    cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 16777216 || All params: 3517190144 || Trainable %: 0.477006226934315\n"
     ]
    }
   ],
   "source": [
    "# Enable gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce the memory consumption during the backward pas. Instead of storing all intermediate activations in the forward pass (which is what's typically done to compute gradients in the backward pass), gradient checkpointing stores only a subset of them\n",
    "base_model.gradient_checkpointing_enable() \n",
    "\n",
    "# Prepare the model for k-bit training . Applies some preprocessing to the model to prepare it for training.\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "qlora_config = LoraConfig(\n",
    "    r=32, #The rank of decomposition r is << min(d,k). The default of r is 8.\n",
    "    lora_alpha=32,#∆W is scaled by α/r where α is a constant. When optimizing with Adam, tuning α is similar as tuning the learning rate.\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], #Modules to Apply LoRA to target_modules. You can select specific modules to fine-tune.\n",
    "    lora_dropout=0.05,#Dropout Probability for LoRA Layers #to reduce overfitting\n",
    "    bias=\"none\", #Bias Type for Lora. Bias can be ‘none’, ‘all’ or ‘lora_only’. If ‘all’ or ‘lora_only’, the corresponding biases will be updated during training. \n",
    "    task_type= \"CAUSAL_LM\", #Task Type\n",
    "    )\n",
    "\n",
    "base_model_qlora = get_peft_model(base_model, qlora_config)\n",
    "\n",
    "# Print the number of trainable parameters in the model\n",
    "print_trainable_parameters(base_model_qlora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Arguments\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size = batch_size ,     # Specifies the batch size for training on each device (GPU).\n",
    "    #auto_find_batch_size=True,      # Uncommenting this would let the library automatically find an optimal batch size.\n",
    "    gradient_accumulation_steps=2,   # Number of forward and backward passes to accumulate gradients before performing an optimizer step.\n",
    "    # This effectively multiplies the batch size by this number without increasing memory usage.\n",
    "    num_train_epochs=epochs,              # Specifies the total number of training epochs.\n",
    "    learning_rate=2e-4,              # Specifies the learning rate for the optimizer.\n",
    "    fp16=True,     # Enables mixed precision training (fp16) which can speed up training and reduce memory usage.\n",
    "    save_total_limit=3,              # Limits the total number of model checkpoints saved. Only the last 3 checkpoints are saved.\n",
    "    logging_steps=10,                 # Specifies how often to log training updates. \n",
    "    output_dir=\"results/\",          # Directory where the model checkpoints and training outputs will be saved.\n",
    "    # max_steps = 200 ,                 # Limits the total number of training steps. Training will stop after 80 steps regardless of epochs.\n",
    "    save_strategy='epoch',    # Uncommenting this would change the strategy for saving model checkpoints. 'epoch' means save after each epoch.\n",
    "    optim=\"paged_adamw_8bit\",     # Specifies the optimizer to use. it's set to a specific variant of AdamW.\n",
    "    lr_scheduler_type = 'constant_with_warmup',     # Specifies the learning rate scheduler type. 'cosine' means it uses cosine annealing.\n",
    "    warmup_ratio = 0.05,           # Specifies the ratio of total steps for the learning rate warmup phase.\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/218 01:22 < 2:27:38, 0.02 it/s, Epoch 0.03/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model_qlora,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    peft_config=qlora_config,\n",
    "    max_seq_length=None,\n",
    "    args=training_arguments,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['report', 'summary', 'length'],\n",
       "        num_rows: 3476\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['report', 'summary', 'length'],\n",
       "        num_rows: 155\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['report', 'summary', 'length'],\n",
       "        num_rows: 169\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
